{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sentence_pattern_list(input_pat):\n",
    "    pattern = []\n",
    "    final = []\n",
    "    for i in input_pat:\n",
    "        if i != '':\n",
    "            pattern.append(i)\n",
    "        else:\n",
    "            final.append(pattern.copy())\n",
    "            pattern.clear()\n",
    "\n",
    "    # Last one\n",
    "    final.append(pattern)\n",
    "    return final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# English\n",
    "english_corpus = open('o.txt', 'r').read().strip('\\n').split('\\n')\n",
    "english_corpus = create_sentence_pattern_list(english_corpus)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chinese\n",
    "chinese_corpus = open('UM-Corpus.ch.200k.tagged.txt', 'r').read().split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "aligns = open('align.final.200k', 'r').read().split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# english_corpus = english_corpus[:5]\n",
    "# chinese_corpus = chinese_corpus[:5]\n",
    "# aligns = aligns[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict, OrderedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pattern_pos(sent1, sent2):\n",
    "    if not isinstance(sent1, list):\n",
    "        sent1 = sent1.split()\n",
    "\n",
    "    if not isinstance(sent2, list):\n",
    "        sent2 = sent2.split()\n",
    "\n",
    "    if len(sent1) < len(sent2):\n",
    "        sent1, sent2 = sent2, sent1\n",
    "\n",
    "    # sent1 is the whole sentence\n",
    "    # sent2 is the sub sentence\n",
    "\n",
    "    count = 0\n",
    "    n = len(sent2)\n",
    "    for i in range(len(sent1)):\n",
    "        count = 0\n",
    "        for j in range(n):\n",
    "            if sent1[i] == sent2[j]:\n",
    "                count += 1\n",
    "                i += 1\n",
    "                if count == n:\n",
    "                    return (i - n, i)\n",
    "            else:\n",
    "                i -= count\n",
    "                break\n",
    "    return (-1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from orderedset import OrderedSet "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_ch_grammar(ch_pat):\n",
    "    ch_grammar = ch_pat.split(' ')\n",
    "    ch_grammar = [cg.split('_')[1] for cg in ch_grammar if '_' in cg]\n",
    "    ch_grammar = [cg for cg in ch_grammar if cg == 'V' or cg == 'P' or cg == 'N']\n",
    "    \n",
    "    if ch_grammar == ['V', 'V']:\n",
    "        ch_grammar = 'V v'\n",
    "    else:\n",
    "        ch_grammar = OrderedSet(ch_grammar)\n",
    "        ch_grammar = ' '.join(ch_grammar).lower().replace('v', 'V')\n",
    "\n",
    "    return ch_grammar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "index = 0\n",
    "en_ch_pat = defaultdict(lambda:defaultdict(list))\n",
    "\n",
    "for english, chinese, align in zip(english_corpus, chinese_corpus, aligns):\n",
    "    en_sen = english[0].split()\n",
    "    ch_sen = chinese.split()\n",
    "    align = align.split()\n",
    "    # 因為連結會有多個選擇只選align最後那個\n",
    "    en_ch = OrderedDict()\n",
    "    \n",
    "   \n",
    "        try:\n",
    "            for a in align:\n",
    "                en_pos, ch_pos = a.split('-')\n",
    "                en_pos = int(en_pos)\n",
    "                ch_pos = int(ch_pos)\n",
    "                en = en_sent[en_pos]\n",
    "                ch = ch_sent[ch_pos]\n",
    "                en_ch[index, en_pos, en] = ch\n",
    "                index += 1\n",
    "\n",
    "            for _ in english[1:]:\n",
    "                _, en_grammar, en_pat = _.split('\\t')\n",
    "                en_grammar = re.sub('about|in|on|to|for|with|of|as', 'p', en_grammar)\n",
    "                start, end = pattern_pos(en_sent, en_pat)\n",
    "                ch_pat = \"\"\n",
    "                for en, ch_term in en_ch.items():\n",
    "                    _, en_pos, en_term = en\n",
    "                    if en_pos >= start and en_pos < end:\n",
    "                        ch_pat += \"%s \" % ch_term\n",
    "                    elif en_pos >= end:\n",
    "                        break\n",
    "                if '_V' in ch_pat:\n",
    "                    ch_grammar = extract_ch_grammar(ch_pat)\n",
    "                    noisy_channel_pattern = \"%s | %s\" % (en_pat, ch_pat)\n",
    "                    en_ch_pat[en_grammar][ch_grammar].append(noisy_channel_pattern)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(\"line %d: %s\" % (count, str(e)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import operator\n",
    "from pprint import pprint\n",
    "\n",
    "PRONS = set([line.strip('\\n') for line in open('prons.txt')])\n",
    "\n",
    "with open('HiFreWords') as f:\n",
    "    HiFreWords = set(f.readline().split('\\t'))\n",
    "\n",
    "def compute_score(word, sent):\n",
    "    global PRONS\n",
    "    global HiFreWords\n",
    "\n",
    "    word = word.lower()\n",
    "    sent = sent.lower().split()\n",
    "    length = len(sent)\n",
    "\n",
    "    locationOfWord = -1 if word not in sent else sent.index(word)\n",
    "    hiFreWordsScore = len([w for w in sent if w not in HiFreWords])\n",
    "    pronsScore = len([w for w in sent if w in PRONS])\n",
    "\n",
    "    return locationOfWord - hiFreWordsScore - pronsScore\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pattern(input_pat):\n",
    "    _sum = 0\n",
    "    stddev = 0.0\n",
    "    k0 = 2\n",
    "\n",
    "    N = len(en_ch_pat[input_pat])\n",
    "    if N == 0:\n",
    "        return \"NO RESULT\"\n",
    "    \n",
    "    for k, v in en_ch_pat[input_pat].items():\n",
    "        _sum += len(v)\n",
    "    avg = _sum / N\n",
    "\n",
    "    print(\"%s (%d)\" % (input_pat, _sum))\n",
    "\n",
    "    for k, v in en_ch_pat[input_pat].items():\n",
    "        stddev += (len(v) - avg) ** 2\n",
    "    stddev = math.sqrt(stddev / N - 1)\n",
    "    \n",
    "    final_result = {}\n",
    "    \n",
    "    # Filter good grammar\n",
    "    for grammar, sentences in en_ch_pat[input_pat].items():\n",
    "        best_sentences = [(-999.9,''), (-999.9,''), (-999.9,'')]\n",
    "        freqi = len(sentences)\n",
    "        strength = (freqi - avg) / stddev\n",
    "        if not strength > k0:\n",
    "            continue\n",
    "\n",
    "        # Find Good Dictionary Example\n",
    "        for sentence in sentences:\n",
    "            score = compute_score(input_pat, sentence)\n",
    "            if score > best_sentences[0][0]:\n",
    "                best_sentences.pop(0)\n",
    "                best_sentences.append((score, sentence))\n",
    "                best_sentences.sort()\n",
    "\n",
    "        final_result[(grammar, freqi)] = best_sentences\n",
    "\n",
    "    # Print the result\n",
    "    for key in sorted(final_result, key=lambda x: x[1], reverse=True):\n",
    "        values = final_result[key]\n",
    "        print('-> %s (%d)' % (key[0], key[1]))\n",
    "        for value in values:\n",
    "            en, ch = value[1].split(\" | \")\n",
    "            print('     %s %s' % (en, ch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "V n (336636)\n",
      "-> V (139139)\n",
      "     explain everything 說明_V \n",
      "     include abdominal discomfort 包括_V \n",
      "     leave any fingerprints 留下_V \n"
     ]
    }
   ],
   "source": [
    "get_pattern('V n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "V p n (98533)\n",
      "-> V (31454)\n",
      "     ask for an extension 要求_V \n",
      "     is distributed in human lungs 分佈_V \n",
      "     look for a credit card 找_V \n"
     ]
    }
   ],
   "source": [
    "get_pattern('V p n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
